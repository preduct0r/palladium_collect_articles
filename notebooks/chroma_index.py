import asyncio
import json
import os
from typing import List, Dict, Optional
import boto3
from dotenv import load_dotenv
from langchain_community.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.schema.document import Document

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–µ—Ä–∞
embedder = OpenAIEmbeddings(
    model="intfloat/multilingual-e5-large-instruct", 
    base_url="http://localhost:8080/v1",
    api_key="EMPTY"
)

def _get_s3_client() -> boto3.client:
    load_dotenv()
    endpoint_url = os.getenv("S3_ENDPOINT_URL")
    # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –¥–≤—É—Ö —Å—Ö–µ–º –∏–º–µ–Ω –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
    access_key = os.getenv("S3_ACCESS_KEY_ID") or os.getenv("AWS_ACCESS_KEY_ID")
    secret_key = os.getenv("S3_SECRET_ACCESS_KEY") or os.getenv("AWS_SECRET_ACCESS_KEY")
    session_token = os.getenv("S3_SESSION_TOKEN") or os.getenv("AWS_SESSION_TOKEN")
    region = os.getenv("AWS_REGION") or os.getenv("AWS_DEFAULT_REGION")

    client_kwargs: Dict[str, Optional[str]] = {}
    if endpoint_url:
        client_kwargs["endpoint_url"] = endpoint_url
    if region:
        client_kwargs["region_name"] = region
    if access_key and secret_key:
        client_kwargs["aws_access_key_id"] = access_key
        client_kwargs["aws_secret_access_key"] = secret_key
    if session_token:
        client_kwargs["aws_session_token"] = session_token

    return boto3.client("s3", **client_kwargs)

def _list_s3_json_keys(s3_client: boto3.client, bucket: str, prefix: Optional[str] = None) -> List[str]:
    keys: List[str] = []
    paginator = s3_client.get_paginator("list_objects_v2")
    for page in paginator.paginate(Bucket=bucket, Prefix=prefix or ""):
        for obj in page.get("Contents", []):
            key = obj.get("Key")
            if key and key.endswith(".json"):
                keys.append(key)
    return keys

def _read_json_from_s3(s3_client: boto3.client, bucket: str, key: str) -> Dict:
    resp = s3_client.get_object(Bucket=bucket, Key=key)
    body = resp["Body"].read()
    return json.loads(body.decode("utf-8"))

def load_texts_from_s3(bucket_name: str, prefix: Optional[str] = None) -> List[str]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–µ–∫—Å—Ç—ã –∏–∑ S3 –±–∞–∫–µ—Ç–∞"""
    # s3 = _get_s3_client()
    
    # print(f"üì¶ –ß—Ç–µ–Ω–∏–µ JSON —Ñ–∞–π–ª–æ–≤ –∏–∑ S3: bucket='{bucket_name}', prefix='{prefix or ''}'")
    # keys = _list_s3_json_keys(s3, bucket_name, prefix)
    
    # if not keys:
    #     print("‚ùå –í S3 –Ω–µ –Ω–∞–π–¥–µ–Ω–æ JSON —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    #     return []
    
    # print(f"üìÑ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(keys)}")
    
    # texts = []
    # for idx, key in enumerate(keys, start=1):
    #     try:
    #         data = _read_json_from_s3(s3, bucket_name, key)
    #         chunks = data.get("content", {}).get("chunks", [])
    #         if not chunks:
    #             print(f"‚ö†Ô∏è –í —Ñ–∞–π–ª–µ {key} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —á–∞–Ω–∫–æ–≤, –ø—Ä–æ–ø—É—Å–∫")
    #             continue
            
    #         print(f"‚û°Ô∏è [{idx}/{len(keys)}] –§–∞–π–ª: {key}, —á–∞–Ω–∫–æ–≤: {len(chunks)}")
            
    #         # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
    #         for chunk in chunks:
    #             if chunk.get("text"):
    #                 texts.append(chunk["text"])
                    
    #     except Exception as e:
    #         print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {key}: {e}")
    #         continue
    
    # print(f"‚úÖ –í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤: {len(texts)}")


    # with open("texts.json", "r") as f:
    #     texts = json.load(f)

    # return [x for x in texts if len(x) > 300]
    
    with open("notebooks/test_retriever.txt", "r") as f:
        texts = f.readlines()
    return texts


# ================================
# Create vectorstore for full text chunks
vectorstore = Chroma(collection_name="relevant_data", embedding_function=embedder)

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏–∑ S3
load_dotenv()
bucket_name = "palladium-md-to-chunks"
prefix = os.getenv("S3_PREFIX")

print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ S3...")
texts = load_texts_from_s3(bucket_name, prefix)

if not texts:
    print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ–∫—Å—Ç—ã –∏–∑ S3")
    exit(1)

# Convert text chunks to Document objects
documents = []
for i, text_chunk in enumerate(texts):
    # Create Document with metadata
    doc = Document(
        page_content=text_chunk,
        metadata={
            "chunk_id": i,
            "source": f"s3://{bucket_name}/{prefix or ''}"
        }
    )
    documents.append(doc)

print(f"üìö –°–æ–∑–¥–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}")

# Add documents to vectorstore in batches
print("üîÑ –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ...")

# ChromaDB has a batch size limit, so we'll add documents in smaller batches
batch_size = 1000  # Safe batch size for ChromaDB
total_docs = len(documents)

for i in range(0, total_docs, batch_size):
    batch = documents[i:i + batch_size]
    batch_num = i // batch_size + 1
    total_batches = (total_docs - 1) // batch_size + 1
    
    print(f"üìù –î–æ–±–∞–≤–ª—è–µ–º –±–∞—Ç—á {batch_num}/{total_batches} ({len(batch)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)...")
    
    try:
        vectorstore.add_documents(batch)
        print(f"‚úÖ –ë–∞—Ç—á {batch_num} —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –±–∞—Ç—á–∞ {batch_num}: {e}")
        continue

print("‚úÖ –í—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ Chroma")

# Create retriever
retriever = vectorstore.as_retriever(search_kwargs={"k": 10})

# ================================
# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞
print("\nüîç –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–µ—Ç—Ä–∏–≤–µ—Ä...")

def test_retriever(query: str, k: int = 5):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞"""
    print(f"\nüìù –ó–∞–ø—Ä–æ—Å: '{query}'")
    
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º retriever –¥–ª—è –ø–æ–∏—Å–∫–∞
        retriever_test = vectorstore.as_retriever(search_kwargs={"k": k})
        results = retriever_test.get_relevant_documents(query)
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:")
        print("=" * 70)
        
        for i, doc in enumerate(results, 1):
            print(f"\n{i}. Chunk ID: {doc.metadata.get('chunk_id', 'N/A')}")
            print(f"Source: {doc.metadata.get('source', 'N/A')}")
            print(f"–¢–µ–∫—Å—Ç (–ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤): {doc.page_content[:200]}...")
            print("-" * 70)
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞: {e}")

# –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
test_queries = [
    # "palladium refining",
    "–≤–∫—É—Å–Ω–æ–µ –º–æ—Ä–æ–∂–µ–Ω–æ–µ"
    # "–º–∏–∫—Ä–æ—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–µ—Ç–∞–ª–ª–æ–≤",
    # "mechanical properties"
]

# –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å –ø–µ—Ä–≤—ã–º –∑–∞–ø—Ä–æ—Å–æ–º
if test_queries:
    test_retriever(test_queries[0], k=3)
    
print(f"\nüéâ –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ Chroma –≥–æ—Ç–æ–≤–æ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")
print(f"üìä –í—Å–µ–≥–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}")