import asyncio
import json
import os
import shutil
import tarfile
from typing import List, Dict, Optional
import boto3
from dotenv import load_dotenv
from langchain_community.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.schema.document import Document

# Configuration
PERSIST_DIRECTORY = "/home/ubuntu/kotov_projects/palladium_collect_articles/chroma_db"
COLLECTION_NAME = "relevant_data"

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–µ—Ä–∞
embedder = OpenAIEmbeddings(
    model="Qwen/Qwen3-Embedding-8B", 
    base_url="http://localhost:8090/v1",
    api_key="EMPTY"
)

def _get_s3_client() -> boto3.client:
    load_dotenv()
    endpoint_url = os.getenv("S3_ENDPOINT_URL")
    # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –¥–≤—É—Ö —Å—Ö–µ–º –∏–º–µ–Ω –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
    access_key = os.getenv("S3_ACCESS_KEY_ID") or os.getenv("AWS_ACCESS_KEY_ID")
    secret_key = os.getenv("S3_SECRET_ACCESS_KEY") or os.getenv("AWS_SECRET_ACCESS_KEY")
    session_token = os.getenv("S3_SESSION_TOKEN") or os.getenv("AWS_SESSION_TOKEN")
    region = os.getenv("AWS_REGION") or os.getenv("AWS_DEFAULT_REGION")

    client_kwargs: Dict[str, Optional[str]] = {}
    if endpoint_url:
        client_kwargs["endpoint_url"] = endpoint_url
    if region:
        client_kwargs["region_name"] = region
    if access_key and secret_key:
        client_kwargs["aws_access_key_id"] = access_key
        client_kwargs["aws_secret_access_key"] = secret_key
    if session_token:
        client_kwargs["aws_session_token"] = session_token

    return boto3.client("s3", **client_kwargs)

def _list_s3_json_keys(s3_client: boto3.client, bucket: str, prefix: Optional[str] = None) -> List[str]:
    keys: List[str] = []
    paginator = s3_client.get_paginator("list_objects_v2")
    for page in paginator.paginate(Bucket=bucket, Prefix=prefix or ""):
        for obj in page.get("Contents", []):
            key = obj.get("Key")
            if key and key.endswith(".json"):
                keys.append(key)
    return keys

def _read_json_from_s3(s3_client: boto3.client, bucket: str, key: str) -> Dict:
    resp = s3_client.get_object(Bucket=bucket, Key=key)
    body = resp["Body"].read()
    return json.loads(body.decode("utf-8"))

def load_texts_from_s3(bucket_name: str, prefix: Optional[str] = None) -> List[str]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–µ–∫—Å—Ç—ã –∏–∑ JSON —Ñ–∞–π–ª–∞"""
    
    json_file_path = "/home/ubuntu/kotov_projects/texts.json"
    
    print(f"üì¶ –ß—Ç–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞: {json_file_path}")
    
    if not os.path.exists(json_file_path):
        print(f"‚ùå –§–∞–π–ª {json_file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return []
    
    try:
        with open(json_file_path, "r", encoding="utf-8") as f:
            texts = json.load(f)
        
        print(f"üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞: {len(texts)}")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–µ–∫—Å—Ç—ã –ø–æ –¥–ª–∏–Ω–µ (–±–æ–ª—å—à–µ 300 —Å–∏–º–≤–æ–ª–æ–≤)
        filtered_texts = [text for text in texts if isinstance(text, str) and len(text) > 300]
        
        print(f"‚úÖ –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (>300 —Å–∏–º–≤–æ–ª–æ–≤): {len(filtered_texts)} —Ç–µ–∫—Å—Ç–æ–≤")
        
        return filtered_texts
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {json_file_path}: {e}")
        return []


def create_or_load_vectorstore(persist_directory: str = PERSIST_DIRECTORY, 
                               collection_name: str = COLLECTION_NAME) -> Chroma:
    """–°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤–æ–µ –∏–ª–∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ"""
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
    os.makedirs(persist_directory, exist_ok=True)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ –∏–Ω–¥–µ–∫—Å
    if os.path.exists(persist_directory) and os.listdir(persist_directory):
        print(f"üìÇ –ù–∞–π–¥–µ–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –≤ {persist_directory}")
        print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å...")
        
        vectorstore = Chroma(
            collection_name=collection_name,
            embedding_function=embedder,
            persist_directory=persist_directory
        )
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º –∏–Ω–¥–µ–∫—Å–µ
        collection = vectorstore._collection
        count = collection.count()
        print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∏–Ω–¥–µ–∫—Å–∞")
        
        return vectorstore
    else:
        print(f"üÜï –°–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –≤ {persist_directory}")
        
        vectorstore = Chroma(
            collection_name=collection_name,
            embedding_function=embedder,
            persist_directory=persist_directory
        )
        
        return vectorstore

def save_index_info(persist_directory: str, metadata: Dict):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–± –∏–Ω–¥–µ–∫—Å–µ"""
    info_file = os.path.join(persist_directory, "index_info.json")
    with open(info_file, "w", encoding="utf-8") as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    print(f"üíæ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∏–Ω–¥–µ–∫—Å–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {info_file}")

def load_index_info(persist_directory: str) -> Optional[Dict]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–± –∏–Ω–¥–µ–∫—Å–µ"""
    info_file = os.path.join(persist_directory, "index_info.json")
    if os.path.exists(info_file):
        with open(info_file, "r", encoding="utf-8") as f:
            return json.load(f)
    return None

def package_index_for_transfer(persist_directory: str, 
                               output_path: str = "chroma_index.tar.gz") -> str:
    """–£–ø–∞–∫–æ–≤—ã–≤–∞–µ—Ç –∏–Ω–¥–µ–∫—Å –≤ –∞—Ä—Ö–∏–≤ –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –Ω–∞ –¥—Ä—É–≥—É—é –º–∞—à–∏–Ω—É"""
    print(f"üì¶ –£–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å –∏–∑ {persist_directory} –≤ {output_path}")
    
    if not os.path.exists(persist_directory):
        raise FileNotFoundError(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {persist_directory} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    
    with tarfile.open(output_path, "w:gz") as tar:
        tar.add(persist_directory, arcname="chroma_db")
    
    file_size = os.path.getsize(output_path) / (1024 * 1024)  # MB
    print(f"‚úÖ –ò–Ω–¥–µ–∫—Å —É–ø–∞–∫–æ–≤–∞–Ω –≤ {output_path} (—Ä–∞–∑–º–µ—Ä: {file_size:.2f} MB)")
    
    return output_path

def extract_index_from_package(archive_path: str, 
                               target_directory: str = PERSIST_DIRECTORY) -> str:
    """–†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ—Ç –∏–Ω–¥–µ–∫—Å –∏–∑ –∞—Ä—Ö–∏–≤–∞"""
    print(f"üì¶ –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å –∏–∑ {archive_path} –≤ {target_directory}")
    
    if not os.path.exists(archive_path):
        raise FileNotFoundError(f"–ê—Ä—Ö–∏–≤ {archive_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    # –£–¥–∞–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
    if os.path.exists(target_directory):
        shutil.rmtree(target_directory)
    
    # –°–æ–∑–¥–∞–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    parent_dir = os.path.dirname(target_directory)
    os.makedirs(parent_dir, exist_ok=True)
    
    with tarfile.open(archive_path, "r:gz") as tar:
        tar.extractall(path=parent_dir)
    
    print(f"‚úÖ –ò–Ω–¥–µ–∫—Å —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω –≤ {target_directory}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏–Ω–¥–µ–∫—Å–µ
    info = load_index_info(target_directory)
    if info:
        print(f"üìã –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∏–Ω–¥–µ–∫—Å–µ:")
        for key, value in info.items():
            print(f"   {key}: {value}")
    
    return target_directory

def test_retriever(vectorstore: Chroma, query: str, k: int = 5):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞"""
    print(f"\nüìù –ó–∞–ø—Ä–æ—Å: '{query}'")
    
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º retriever –¥–ª—è –ø–æ–∏—Å–∫–∞
        retriever_test = vectorstore.as_retriever(search_kwargs={"k": k})
        results = retriever_test.get_relevant_documents(query)
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:")
        print("=" * 70)
        
        for i, doc in enumerate(results, 1):
            print(f"\n{i}. Chunk ID: {doc.metadata.get('chunk_id', 'N/A')}")
            print(f"Source: {doc.metadata.get('source', 'N/A')}")
            print(f"–¢–µ–∫—Å—Ç (–ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤): {doc.page_content[:200]}...")
            print("-" * 70)
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞: {e}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞"""
    
    # ================================
    # Create or load vectorstore
    
    vectorstore = create_or_load_vectorstore()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –¥–æ–±–∞–≤–ª—è—Ç—å –Ω–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    collection = vectorstore._collection
    existing_count = collection.count()
    
    if existing_count == 0:
        print("üìù –ò–Ω–¥–µ–∫—Å –ø—É—Å—Ç–æ–π, –¥–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏–∑ JSON —Ñ–∞–π–ª–∞
        load_dotenv()
        bucket_name = "palladium-md-to-chunks"  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        prefix = os.getenv("S3_PREFIX")  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏

        print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Ñ–∞–π–ª–∞...")
        texts = load_texts_from_s3(bucket_name, prefix)

        if not texts:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ–∫—Å—Ç—ã –∏–∑ JSON —Ñ–∞–π–ª–∞")
            exit(1)

        # Convert text chunks to Document objects
        documents = []
        for i, text_chunk in enumerate(texts):
            # Create Document with metadata
            doc = Document(
                page_content=text_chunk,
                metadata={
                    "chunk_id": i,
                    "source": "texts.json",
                    "text_length": len(text_chunk)
                }
            )
            documents.append(doc)

        print(f"üìö –°–æ–∑–¥–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}")

        # Add documents to vectorstore in batches
        print("üîÑ –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ...")

        # ChromaDB has a batch size limit, so we'll add documents in smaller batches
        batch_size = 1000  # Safe batch size for ChromaDB
        total_docs = len(documents)

        for i in range(0, total_docs, batch_size):
            batch = documents[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (total_docs - 1) // batch_size + 1
            
            print(f"üìù –î–æ–±–∞–≤–ª—è–µ–º –±–∞—Ç—á {batch_num}/{total_batches} ({len(batch)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)...")
            
            try:
                vectorstore.add_documents(batch)
                print(f"‚úÖ –ë–∞—Ç—á {batch_num} —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –±–∞—Ç—á–∞ {batch_num}: {e}")
                continue

        print("‚úÖ –í—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ Chroma")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–± –∏–Ω–¥–µ–∫—Å–µ
        import datetime
        metadata = {
            "created_at": datetime.datetime.now().isoformat(),
            "total_documents": len(documents),
            "embedding_model": "Qwen/Qwen3-Embedding-8B",
            "collection_name": COLLECTION_NAME,
            "source_file": "texts.json",
            "filtered_min_length": 300
        }
        save_index_info(PERSIST_DIRECTORY, metadata)
        
    else:
        print(f"üìä –ò—Å–ø–æ–ª—å–∑—É—é —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å —Å {existing_count} –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏")

    # Create retriever
    retriever = vectorstore.as_retriever(search_kwargs={"k": 10})

    # ================================
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞
    print("\nüîç –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–µ—Ç—Ä–∏–≤–µ—Ä...")

    # –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    test_queries = [
        # "palladium refining",
        "chocolate ice cream"
        # "–º–∏–∫—Ä–æ—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–µ—Ç–∞–ª–ª–æ–≤",
        # "mechanical properties"
    ]

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å –ø–µ—Ä–≤—ã–º –∑–∞–ø—Ä–æ—Å–æ–º
    if test_queries:
        test_retriever(vectorstore, test_queries[0], k=30)
        
    print(f"\nüéâ –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ Chroma –≥–æ—Ç–æ–≤–æ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")
    print(f"üìä –í—Å–µ–≥–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {existing_count}")
    print(f"üíæ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {PERSIST_DIRECTORY}")
    
    # –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º —É–ø–∞–∫–æ–≤–∞—Ç—å –∏–Ω–¥–µ–∫—Å –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏
    response = input("\nü§î –•–æ—Ç–∏—Ç–µ —É–ø–∞–∫–æ–≤–∞—Ç—å –∏–Ω–¥–µ–∫—Å –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –Ω–∞ –¥—Ä—É–≥—É—é –º–∞—à–∏–Ω—É? (y/n): ").strip().lower()
    if response == 'y':
        output_path = "chroma_index.tar.gz"  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        # output_path = input("üìÅ –í–≤–µ–¥–∏—Ç–µ –ø—É—Ç—å –¥–ª—è –∞—Ä—Ö–∏–≤–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: chroma_index.tar.gz): ").strip()
        # if not output_path:
        #     output_path = "chroma_index.tar.gz"
        
        try:
            package_index_for_transfer(PERSIST_DIRECTORY, output_path)
            print(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –¢–µ–ø–µ—Ä—å –≤—ã –º–æ–∂–µ—Ç–µ –ø–µ—Ä–µ–¥–∞—Ç—å —Ñ–∞–π–ª {output_path} –Ω–∞ –¥—Ä—É–≥—É—é –º–∞—à–∏–Ω—É")
            print("üìã –î–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –Ω–∞ –¥—Ä—É–≥–æ–π –º–∞—à–∏–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é extract_index_from_package()")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–ø–∞–∫–æ–≤–∫–µ: {e}")

if __name__ == "__main__":
    main()