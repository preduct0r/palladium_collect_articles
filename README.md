# Сборщик научных статей

Этот скрипт представляет собой краулер, который собирает научные статьи и информацию о них, начиная с одной "семенной" статьи (заданной через DOI). Он рекурсивно обходит граф цитирований, скачивает PDF-файлы статей (используя открытые источники и Sci-Hub) и сохраняет их в S3-совместимое хранилище (Yandex Object Storage), а метаинформацию сохраняет в локальную базу данных SQLite.

## Как это работает

Весь процесс можно описать так:

**Старт с DOI → Получить метаданные из OpenAlex → Попытаться скачать PDF → Сохранить PDF в S3 → Сохранить метаданные в локальную БД → Найти все статьи, на которые ссылается исходная (references) → Добавить их в очередь → Повторить для каждой статьи из очереди.**

Это продолжается до тех пор, пока не будет достигнута заданная глубина обхода или общее количество обработанных статей.

### 1. Запуск и настройка

Скрипт запускается из командной строки и принимает несколько аргументов:

-   `--doi`: **Обязательный.** DOI-идентификатор или URL статьи, с которой начинается обход.
-   `--db`: Путь к файлу базы данных SQLite (по умолчанию: `./articles.db`).
-   `--mailto`: Ваш email для вежливого использования API OpenAlex (рекомендуется указывать).
-   `--max-depth`: Глубина обхода графа цитирований. `0` — только сама статья, `1` — статья и те, что на неё ссылаются, и т.д. (по умолчанию: `1`).
-   `--max-total`: Максимальное количество статей для обработки за один запуск.
-   `--per-parent-limit`: Максимальное количество цитирующих статей для обработки для каждой родительской статьи (по умолчанию: `50`).
-   `--sleep`: Пауза между запросами к API в секундах (по умолчанию: `1.0`).
-   `--no-scihub`: Флаг, отключающий попытку скачивания PDF через Sci-Hub, если статья не в открытом доступе.

### 2. Настройка окружения

Перед запуском необходимо создать файл `.env` в корне проекта с настройками S3:

```env
# S3 Configuration (Yandex Object Storage)
S3_ENDPOINT_URL=https://storage.yandexcloud.net
S3_BUCKET_NAME=palladium-articles
S3_ACCESS_KEY_ID=ваш_access_key
S3_SECRET_ACCESS_KEY=ваш_secret_key
```

### 3. Инициализация базы данных

Перед началом работы скрипт создаёт или подключается к локальной базе данных SQLite. В базе создаются две основные таблицы:

1.  `articles`: Хранит всю информацию о каждой статье: DOI, ID в OpenAlex, заголовок, год, статус открытого доступа (Open Access), количество цитирований, S3 ключ для PDF-файла и т.д. `DOI` является первичным ключом.
2.  `relations`: Хранит связи между статьями (граф цитирований). Содержит поля `from_doi` (кого цитируют) и `to_doi` (кто цитирует).

### 4. Обход графа цитирований

-   Скрипт использует алгоритм **поиска в ширину (BFS)** для обхода графа.
-   Создается очередь, в которую помещаются статьи для обработки.
-   Обход начинается с "семенной" статьи и продолжается, пока очередь не опустеет или не будет достигнут лимит `--max-total`.
-   Используется множество `visited` для отслеживания уже обработанных DOI, чтобы избежать повторной работы и зацикливания.

### 5. Обработка одной статьи

Для каждой статьи выполняются следующие шаги:

1.  **Получение метаданных**: Отправляется запрос к API OpenAlex для получения полной информации о статье по её DOI.
2.  **Загрузка PDF**: Скрипт пытается скачать PDF:
    -   Сначала он проверяет прямую ссылку на PDF в данных OpenAlex (если статья в открытом доступе).
    -   Если это не удалось и опция `--no-scihub` не установлена, скрипт использует библиотеку `sci_hub` для поиска и скачивания PDF через Sci-Hub.
    -   **PDF-файлы сохраняются напрямую в S3 бакет**, локальные файлы не создаются.
3.  **Сохранение в БД**: Информация о статье, включая S3 ключ для PDF-файла и источник (`openalex` или `scihub`), сохраняется в локальную таблицу `articles`.
4.  **Поиск ссылок (references)**: Скрипт извлекает список работ, на которые ссылается текущая статья (`referenced_works`), и получает по ним метаданные.
5.  **Добавление дочерних статей в очередь**: Каждая найденная цитирующая статья ("ребенок") добавляется в базу данных, создается связь в таблице `relations`, и статья добавляется в очередь для дальнейшей обработки, если не превышена глубина обхода.

## Требования

- Python 3.7+
- `requests`
- `boto3` (для работы с S3)
- `python-dotenv` (для загрузки переменных окружения)
- `scihub` (для скачивания закрытых статей)

Установить зависимости:
```bash
pip install -r requirements.txt
```

Или установить вручную:
```bash
pip install requests boto3 python-dotenv scihub
```

## Пример запуска

### Обработка одной статьи:
```bash
python main.py \
    --doi "10.1038/s41586-020-2649-2" \
    --mailto "your.email@example.com" \
    --max-depth 2 \
    --per-parent-limit 10
```

### Параллельная обработка нескольких статей в отдельных процессах:
```bash
python main.py \
    --doi "10.1038/s41586-020-2649-2" "10.1038/srep37043" "10.1126/science.abc123" \
    --mailto "your.email@example.com" \
    --max-depth 1 \
    --per-parent-limit 5 \
    --min-citations 10 \
    --min-year 2015
```

При запуске скрипт:
1. Получит заголовки всех указанных статей
2. Создаст отдельную папку для каждой статьи (например, `Quantum_Computing_Breakthrough_in_Machine_Learning`)
3. **Запустит каждую статью в отдельном процессе** с общей базой данных
4. Сохранит все найденные PDF в соответствующие папки в S3: `palladium-articles/[название_статьи]/`
5. Все статьи будут в одной БД `articles.db`, но с полем `seed_article_title` для разделения по источникам

### 6. Параллельная обработка

При указании нескольких DOI скрипт автоматически использует многопроцессорность:

- **Каждый DOI обрабатывается в отдельном процессе**
- **Каждая статья создает свою собственную папку в S3**
- **Все процессы используют одну общую базу данных** (SQLite WAL mode поддерживает параллельную запись)
- **Статьи разделяются по полю `seed_article_title`** в базе данных

Преимущества параллельной обработки:
- Значительное сокращение времени выполнения при обработке нескольких статей
- Эффективное использование сетевых ресурсов
- Возможность обработки больших объемов данных
- **Единая база данных** для удобного анализа всех собранных данных

## Хранение данных

- **PDF-файлы**: Сохраняются в S3 бакет `palladium-articles` (Yandex Object Storage)
  - Все PDF файлы сохраняются в папку, названную по заголовку исходной (seed) статьи
  - Название папки автоматически очищается от недопустимых символов
  - Пример структуры: `palladium-articles/Deep_Learning_Approach_to_Pattern_Recognition/article1.pdf`
- **Метаданные**: Сохраняются в единую SQLite базу данных (по умолчанию `./articles.db`)
- **Разделение по источникам**: Поле `seed_article_title` показывает, из какой исходной статьи была найдена каждая запись
- **S3 ключи**: В базе данных сохраняются полные S3 пути включая папку (например, `folder_name/article.pdf`)

## Работа с базой данных SQLite

Для просмотра собранных данных используйте команды SQLite:

### Основные запросы для просмотра данных

```bash
# Открыть базу данных
sqlite3 articles.db

# Посмотреть структуру таблиц
.schema

# Количество статей и связей
SELECT COUNT(*) as total_articles FROM articles;
SELECT COUNT(*) as total_relations FROM relations;

# Показать первые 10 статей с их метаданными
SELECT doi, title, year, cited_by_count, distance, seed_article_title FROM articles LIMIT 10;

# Статьи по исходной статье (seed)
SELECT seed_article_title, COUNT(*) as count FROM articles GROUP BY seed_article_title;

# Статьи по глубине обнаружения для конкретной исходной статьи
SELECT distance, COUNT(*) as count FROM articles 
WHERE seed_article_title LIKE '%ваш_поисковый_термин%' 
GROUP BY distance ORDER BY distance;
```

## Индексация S3-чанков в Elasticsearch (векторный поиск)

Скрипт `elastic_create.py` читает JSON-файлы чанков из S3 бакета и индексирует их в Elasticsearch c эмбеддингами для kNN-поиска.

### Переменные окружения

Скрипт использует ключи из `.env` (или переменные окружения процесса):

```env
# S3 (поддерживаются как Yandex S3, так и AWS-переменные)
S3_ENDPOINT_URL=https://storage.yandexcloud.net
S3_BUCKET_NAME=palladium-md-to-chunks     # бакет с готовыми чанками
# Необязательно: ограничить поддиректорией внутри бакета
S3_PREFIX=                                # например: 10.1007_s11664-013-2609-9/

# Креды (любой из форматов)
S3_ACCESS_KEY_ID=...
S3_SECRET_ACCESS_KEY=...
# или AWS_ACCESS_KEY_ID / AWS_SECRET_ACCESS_KEY / AWS_REGION
```

Необязательные параметры:

- `ES_INDEX_NAME` — имя индекса (по умолчанию `scientific_papers`)

### Зависимости и окружение

```bash
conda activate palladium
pip install -r requirements.txt
```

### Запуск индексации

- Через `.env`:
```bash
python elastic_create.py
```

- С переопределением переменных без изменения `.env`:
```bash
S3_BUCKET_NAME=palladium-md-to-chunks python elastic_create.py
```

- Длительный запуск в фоне с логом:
```bash
nohup env S3_BUCKET_NAME=palladium-md-to-chunks python elastic_create.py > index.log 2>&1 &
# смотреть лог
tail -f index.log
```

### Важные замечания

- Текущая реализация при каждом запуске пересоздаёт индекс `scientific_papers` (удаляет и создаёт заново).
  - Если нужно сохранять существующие данные и делать upsert — дайте знать, добавим режим без пересоздания.
- Скрипт ожидает локально работающий Elasticsearch `http://localhost:9200` и сервис эмбеддингов на `http://localhost:8080/v1` с моделью `intfloat/multilingual-e5-large-instruct`. 