# Сборщик научных статей

Этот скрипт представляет собой краулер, который собирает научные статьи и информацию о них, начиная с одной "семенной" статьи (заданной через DOI). Он рекурсивно обходит граф цитирований, скачивает PDF-файлы статей (используя открытые источники и Sci-Hub) и сохраняет их в S3-совместимое хранилище (Yandex Object Storage), а метаинформацию сохраняет в локальную базу данных SQLite.

## Как это работает

Весь процесс можно описать так:

**Старт с DOI → Получить метаданные из OpenAlex → Попытаться скачать PDF → Сохранить PDF в S3 → Сохранить метаданные в локальную БД → Найти все статьи, на которые ссылается исходная (references) → Добавить их в очередь → Повторить для каждой статьи из очереди.**

Это продолжается до тех пор, пока не будет достигнута заданная глубина обхода или общее количество обработанных статей.

### 1. Запуск и настройка

Скрипт запускается из командной строки и принимает несколько аргументов:

-   `--doi`: **Обязательный.** DOI-идентификатор или URL статьи, с которой начинается обход.
-   `--db`: Путь к файлу базы данных SQLite (по умолчанию: `./articles.db`).
-   `--mailto`: Ваш email для вежливого использования API OpenAlex (рекомендуется указывать).
-   `--max-depth`: Глубина обхода графа цитирований. `0` — только сама статья, `1` — статья и те, что на неё ссылаются, и т.д. (по умолчанию: `1`).
-   `--max-total`: Максимальное количество статей для обработки за один запуск.
-   `--per-parent-limit`: Максимальное количество цитирующих статей для обработки для каждой родительской статьи (по умолчанию: `50`).
-   `--sleep`: Пауза между запросами к API в секундах (по умолчанию: `1.0`).
-   `--no-scihub`: Флаг, отключающий попытку скачивания PDF через Sci-Hub, если статья не в открытом доступе.

### 2. Настройка окружения

Перед запуском необходимо создать файл `.env` в корне проекта с настройками S3:

```env
# S3 Configuration (Yandex Object Storage)
S3_ENDPOINT_URL=https://storage.yandexcloud.net
S3_BUCKET_NAME=palladium-articles
S3_ACCESS_KEY_ID=ваш_access_key
S3_SECRET_ACCESS_KEY=ваш_secret_key
```

### 3. Инициализация базы данных

Перед началом работы скрипт создаёт или подключается к локальной базе данных SQLite. В базе создаются две основные таблицы:

1.  `articles`: Хранит всю информацию о каждой статье: DOI, ID в OpenAlex, заголовок, год, статус открытого доступа (Open Access), количество цитирований, S3 ключ для PDF-файла и т.д. `DOI` является первичным ключом.
2.  `relations`: Хранит связи между статьями (граф цитирований). Содержит поля `from_doi` (кого цитируют) и `to_doi` (кто цитирует).

### 4. Обход графа цитирований

-   Скрипт использует алгоритм **поиска в ширину (BFS)** для обхода графа.
-   Создается очередь, в которую помещаются статьи для обработки.
-   Обход начинается с "семенной" статьи и продолжается, пока очередь не опустеет или не будет достигнут лимит `--max-total`.
-   Используется множество `visited` для отслеживания уже обработанных DOI, чтобы избежать повторной работы и зацикливания.

### 5. Обработка одной статьи

Для каждой статьи выполняются следующие шаги:

1.  **Получение метаданных**: Отправляется запрос к API OpenAlex для получения полной информации о статье по её DOI.
2.  **Загрузка PDF**: Скрипт пытается скачать PDF:
    -   Сначала он проверяет прямую ссылку на PDF в данных OpenAlex (если статья в открытом доступе).
    -   Если это не удалось и опция `--no-scihub` не установлена, скрипт использует библиотеку `sci_hub` для поиска и скачивания PDF через Sci-Hub.
    -   **PDF-файлы сохраняются напрямую в S3 бакет**, локальные файлы не создаются.
3.  **Сохранение в БД**: Информация о статье, включая S3 ключ для PDF-файла и источник (`openalex` или `scihub`), сохраняется в локальную таблицу `articles`.
4.  **Поиск ссылок (references)**: Скрипт извлекает список работ, на которые ссылается текущая статья (`referenced_works`), и получает по ним метаданные.
5.  **Добавление дочерних статей в очередь**: Каждая найденная цитирующая статья ("ребенок") добавляется в базу данных, создается связь в таблице `relations`, и статья добавляется в очередь для дальнейшей обработки, если не превышена глубина обхода.

## Требования

- Python 3.7+
- `requests`
- `boto3` (для работы с S3)
- `python-dotenv` (для загрузки переменных окружения)
- `scihub` (для скачивания закрытых статей)

Установить зависимости:
```bash
pip install -r requirements.txt
```

Или установить вручную:
```bash
pip install requests boto3 python-dotenv scihub
```

## Пример запуска

```bash
python main.py \
    --doi "10.1038/s41586-020-2649-2" \
    --mailto "your.email@example.com" \
    --max-depth 2 \
    --per-parent-limit 10
```

При запуске скрипт:
1. Получит заголовок статьи с DOI `10.1038/s41586-020-2649-2`
2. Создаст безопасное имя папки (например, `Quantum_Computing_Breakthrough_in_Machine_Learning`)
3. Сохранит все найденные PDF в эту папку в S3: `palladium-articles/Quantum_Computing_Breakthrough_in_Machine_Learning/`
4. В локальной БД будут сохранены полные S3 пути к файлам

## Хранение данных

- **PDF-файлы**: Сохраняются в S3 бакет `palladium-articles` (Yandex Object Storage)
  - Все PDF файлы сохраняются в папку, названную по заголовку исходной (seed) статьи
  - Название папки автоматически очищается от недопустимых символов
  - Пример структуры: `palladium-articles/Deep_Learning_Approach_to_Pattern_Recognition/article1.pdf`
- **Метаданные**: Сохраняются в локальную SQLite базу данных (по умолчанию `./articles.db`)
- **S3 ключи**: В базе данных сохраняются полные S3 пути включая папку (например, `folder_name/article.pdf`)

## Работа с базой данных SQLite

Для просмотра собранных данных используйте команды SQLite:

### Основные запросы для просмотра данных

```bash
# Открыть базу данных
sqlite3 articles.db

# Посмотреть структуру таблиц
.schema

# Количество статей и связей
SELECT COUNT(*) as total_articles FROM articles;
SELECT COUNT(*) as total_relations FROM relations;

# Показать первые 10 статей с их метаданными
SELECT doi, title, year, cited_by_count, distance FROM articles LIMIT 10;

# Статьи по глубине обнаружения
SELECT distance, COUNT(*) as count FROM articles GROUP BY distance ORDER BY distance;

# Статьи с самой высокой цитируемостью
SELECT title, doi, cited_by_count, year FROM articles ORDER BY cited_by_count DESC LIMIT 10;

# Посмотреть связи между статьями
SELECT a1.title as from_title, a2.title as to_title, r.relation 
FROM relations r 
JOIN articles a1 ON r.from_doi = a1.doi 
JOIN articles a2 ON r.to_doi = a2.doi 
LIMIT 10;

# Статьи с успешно скачанными PDF
SELECT title, pdf_path, source_pdf FROM articles WHERE pdf_path IS NOT NULL LIMIT 10;

# Статистика по источникам PDF
SELECT source_pdf, COUNT(*) as count FROM articles WHERE source_pdf IS NOT NULL GROUP BY source_pdf;

# Выйти из SQLite
.quit
```

### Полезные команды для анализа

```bash
# Экспорт данных в CSV
sqlite3 articles.db -header -csv "SELECT * FROM articles;" > articles.csv
sqlite3 articles.db -header -csv "SELECT * FROM relations;" > relations.csv

# Поиск статей по ключевым словам в заголовке
sqlite3 articles.db "SELECT title, doi, year FROM articles WHERE title LIKE '%machine learning%';"

# Найти самые цитируемые статьи по годам
sqlite3 articles.db "SELECT year, MAX(cited_by_count) as max_citations, title FROM articles WHERE year IS NOT NULL GROUP BY year ORDER BY year DESC;"
``` 